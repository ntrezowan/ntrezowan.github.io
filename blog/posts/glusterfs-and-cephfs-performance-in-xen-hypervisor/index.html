<!DOCTYPE html>
<html lang="en">
  <head>
    <title>
  GlusterFS and CephFS Performance in Xen Hypervisor Â· johndoeyo
</title>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="color-scheme" content="light dark">




<meta name="author" content="John De">
<meta name="description" content="A. Host/Virtual Machines Configuration Link to heading 1. Virtual Machines Link to heading 1.1 GlusterFS VMs Link to heading Hostname gfs1 gfs2 gfs3 Operating System CentOS 6.5 CentOS 6.5 CentOS 6.5 RAM 2 GB 2 GB 2 GB Disk 100 GB 100 GB 100 GB Network Bridged Bridged Bridged IP Address 192.168.26.234 192.168.26.235 192.168.26.236 1.2 CephFS VMs Link to heading Hostname ceph1 ceph2 ceph3 Operating System CentOS 6.5 CentOS 6.">
<meta name="keywords" content="blog,developer,personal">

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="GlusterFS and CephFS Performance in Xen Hypervisor"/>
<meta name="twitter:description" content="A. Host/Virtual Machines Configuration Link to heading 1. Virtual Machines Link to heading 1.1 GlusterFS VMs Link to heading Hostname gfs1 gfs2 gfs3 Operating System CentOS 6.5 CentOS 6.5 CentOS 6.5 RAM 2 GB 2 GB 2 GB Disk 100 GB 100 GB 100 GB Network Bridged Bridged Bridged IP Address 192.168.26.234 192.168.26.235 192.168.26.236 1.2 CephFS VMs Link to heading Hostname ceph1 ceph2 ceph3 Operating System CentOS 6.5 CentOS 6."/>

<meta property="og:title" content="GlusterFS and CephFS Performance in Xen Hypervisor" />
<meta property="og:description" content="A. Host/Virtual Machines Configuration Link to heading 1. Virtual Machines Link to heading 1.1 GlusterFS VMs Link to heading Hostname gfs1 gfs2 gfs3 Operating System CentOS 6.5 CentOS 6.5 CentOS 6.5 RAM 2 GB 2 GB 2 GB Disk 100 GB 100 GB 100 GB Network Bridged Bridged Bridged IP Address 192.168.26.234 192.168.26.235 192.168.26.236 1.2 CephFS VMs Link to heading Hostname ceph1 ceph2 ceph3 Operating System CentOS 6.5 CentOS 6." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://www.example.com/posts/glusterfs-and-cephfs-performance-in-xen-hypervisor/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2014-07-22T01:18:37-05:00" />
<meta property="article:modified_time" content="2014-07-22T01:18:37-05:00" />




<link rel="canonical" href="http://www.example.com/posts/glusterfs-and-cephfs-performance-in-xen-hypervisor/">


<link rel="preload" href="/fonts/forkawesome-webfont.woff2?v=1.2.0" as="font" type="font/woff2" crossorigin>


  
  
  <link rel="stylesheet" href="/css/coder.min.36f76aaf39a14ecf5c3a3c6250dcaf06c238b3d8365d17d646f95cb1874e852b.css" integrity="sha256-NvdqrzmhTs9cOjxiUNyvBsI4s9g2XRfWRvlcsYdOhSs=" crossorigin="anonymous" media="screen" />






  
    
    
    <link rel="stylesheet" href="/css/coder-dark.min.593028e7f7ac55c003b79c230d1cd411bb4ca53b31556c3abb7f027170e646e9.css" integrity="sha256-WTAo5/esVcADt5wjDRzUEbtMpTsxVWw6u38CcXDmRuk=" crossorigin="anonymous" media="screen" />
  



 




<link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

<link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#5bbad5">




<meta name="generator" content="Hugo 0.110.0">





  </head>






<body class="preload-transitions colorscheme-auto">
  
<div class="float-container">
    <a id="dark-mode-toggle" class="colorscheme-toggle">
        <i class="fa fa-adjust fa-fw" aria-hidden="true"></i>
    </a>
</div>


  <main class="wrapper">
    <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">
      johndoeyo
    </a>
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link" href="/posts/">Blog</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>


    <div class="content">
      
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">
            <a class="title-link" href="http://www.example.com/posts/glusterfs-and-cephfs-performance-in-xen-hypervisor/">
              GlusterFS and CephFS Performance in Xen Hypervisor
            </a>
          </h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa fa-calendar" aria-hidden="true"></i>
              <time datetime="2014-07-22T01:18:37-05:00">
                Jul 22, 2014
              </time>
            </span>
            <span class="reading-time">
              <i class="fa fa-clock-o" aria-hidden="true"></i>
              22-minute read
            </span>
          </div>
          
          
          
        </div>
      </header>

      <div class="post-content">
        
        <h2 id="a-hostvirtual-machines-configuration">
  A. Host/Virtual Machines Configuration
  <a class="heading-link" href="#a-hostvirtual-machines-configuration">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<h3 id="1-virtual-machines">
  1. Virtual Machines
  <a class="heading-link" href="#1-virtual-machines">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<h4 id="11-glusterfs-vms">
  1.1 GlusterFS VMs
  <a class="heading-link" href="#11-glusterfs-vms">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h4>
<table>
<thead>
<tr>
<th>Hostname</th>
<th>gfs1</th>
<th>gfs2</th>
<th>gfs3</th>
</tr>
</thead>
<tbody>
<tr>
<td>Operating System</td>
<td>CentOS 6.5</td>
<td>CentOS 6.5</td>
<td>CentOS 6.5</td>
</tr>
<tr>
<td>RAM</td>
<td>2 GB</td>
<td>2 GB</td>
<td>2 GB</td>
</tr>
<tr>
<td>Disk</td>
<td>100 GB</td>
<td>100 GB</td>
<td>100 GB</td>
</tr>
<tr>
<td>Network</td>
<td>Bridged</td>
<td>Bridged</td>
<td>Bridged</td>
</tr>
<tr>
<td>IP Address</td>
<td>192.168.26.234</td>
<td>192.168.26.235</td>
<td>192.168.26.236</td>
</tr>
</tbody>
</table>
<h4 id="12-cephfs-vms">
  1.2 CephFS VMs
  <a class="heading-link" href="#12-cephfs-vms">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h4>
<table>
<thead>
<tr>
<th>Hostname</th>
<th>ceph1</th>
<th>ceph2</th>
<th>ceph3</th>
</tr>
</thead>
<tbody>
<tr>
<td>Operating System</td>
<td>CentOS 6.5</td>
<td>CentOS 6.5</td>
<td>CentOS 6.5</td>
</tr>
<tr>
<td>RAM</td>
<td>2 GB</td>
<td>2 GB</td>
<td>2 GB</td>
</tr>
<tr>
<td>Disk</td>
<td>100 GB</td>
<td>100 GB</td>
<td>100 GB</td>
</tr>
<tr>
<td>Network</td>
<td>Bridged</td>
<td>Bridged</td>
<td>Bridged</td>
</tr>
<tr>
<td>IP Address</td>
<td>192.168.26.237</td>
<td>192.168.26.238</td>
<td>192.168.26.239</td>
</tr>
</tbody>
</table>
<h4 id="13-client-vms">
  1.3 Client VMs
  <a class="heading-link" href="#13-client-vms">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h4>
<table>
<thead>
<tr>
<th>Hostname</th>
<th>client1</th>
<th>client2</th>
</tr>
</thead>
<tbody>
<tr>
<td>Operating System</td>
<td>CentOS 6.5</td>
<td>Ubuntu 12.04</td>
</tr>
<tr>
<td>RAM</td>
<td>2 GB</td>
<td>2 GB</td>
</tr>
<tr>
<td>Disk</td>
<td>100 GB</td>
<td>100 GB</td>
</tr>
<tr>
<td>Network</td>
<td>Bridged</td>
<td>Bridged</td>
</tr>
<tr>
<td>IP Address</td>
<td>192.168.26.240</td>
<td>192.168.26.241</td>
</tr>
</tbody>
</table>
<h3 id="2-host-machines">
  2 Host Machines
  <a class="heading-link" href="#2-host-machines">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<table>
<thead>
<tr>
<th>Hostname</th>
<th>server1</th>
<th>server2</th>
<th>server3</th>
<th>server4</th>
</tr>
</thead>
<tbody>
<tr>
<td>OS</td>
<td>CentOS 6.5</td>
<td>CentOS 6.5</td>
<td>CentOS 6.5</td>
<td>CentOS 6.5</td>
</tr>
<tr>
<td>RAM (DDR3)</td>
<td>4 GB</td>
<td>4 GB</td>
<td>4 GB</td>
<td>4 GB</td>
</tr>
<tr>
<td>Disk (7200 RPM)</td>
<td>300 GB</td>
<td>300 GB</td>
<td>300 GB</td>
<td>300 GB</td>
</tr>
<tr>
<td>Network (1GB)</td>
<td>NAT+Bridged</td>
<td>NAT+Bridged</td>
<td>NAT+Bridged</td>
<td>NAT+Bridged</td>
</tr>
<tr>
<td>IP Address</td>
<td>192.168.26.230</td>
<td>192.168.26.231</td>
<td>192.168.26.232</td>
<td>192.168.26.233</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="b-install-hypervisor-and-configure-vms">
  B. Install Hypervisor and configure VMs
  <a class="heading-link" href="#b-install-hypervisor-and-configure-vms">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<h3 id="1-installing-the-hypervisor">
  1. Installing the hypervisor
  <a class="heading-link" href="#1-installing-the-hypervisor">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<ol>
<li>Install Xen4 CentOS stack from the repository;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># yum install centos-release-xen
</span></span></code></pre></div><ol start="2">
<li>Install Xen itself;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># yum install xen
</span></span><span style="display:flex;"><span># /etc/init.d/xend start
</span></span></code></pre></div><ol start="3">
<li>With the default installation, Xen hypervisor runs above the host linux kernel, so we need to change the boot order of the linux kernel to dom0(Host operating system). The <code>centoso-release-xen</code> installer includes a script called <code>grub-bootxen.sh</code> which will change the host operating systems boot order configuration with a predefined value;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># /usr/bin/grub-bootxen.sh
</span></span></code></pre></div><p>Which will change <code>/boot/grub/grub.conf</code> to the following configuration;</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-v" data-lang="v"><span style="display:flex;"><span># cat /boot/grub/grub.conf | more
</span></span><span style="display:flex;"><span>title <span style="font-weight:bold">CentOS</span> (x.x.xx-x.elx.centos.alt.x86_64) 
</span></span><span style="display:flex;"><span>root (hd0,0) 
</span></span><span style="display:flex;"><span>kernel /xen.gz dom0_mem=1024<span style="font-weight:bold">M</span>,max:1024<span style="font-weight:bold">M</span> loglvl=all guest_loglvl=all 
</span></span><span style="display:flex;"><span><span style="font-weight:bold">module</span> /vmlinuz-x.x.xx-x.elx.centos.alt.x86_64 ro root=/dev/mapper/vg_xen01-lv_root rd_LVM_LV=vg_xen01/lv_swap rd_NO_LUKS <span style="font-weight:bold">KEYBOARDTYPE</span>=pc <span style="font-weight:bold">KEYTABLE</span>=uk rd_NO_MD <span style="font-weight:bold">LANG</span>=en_GB rd_LVM_LV=vg_xen01/lv_root <span style="font-weight:bold">SYSFONT</span>=latarcyrheb-sun16 crashkernel=auto rd_NO_DM rhgb quiet 
</span></span><span style="display:flex;"><span><span style="font-weight:bold">module</span> /initramfs-x.x.xx-x.elx.centos.alt.x86_64.img
</span></span></code></pre></div><ol start="4">
<li>After the reboot, Xen is loaded properly and to verify;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># xm info
</span></span></code></pre></div><ol start="5">
<li>Now we need to install <code>libvirthich</code> which will create an internal NAT network for the virtual machines behind the default network card. To install <code>libvirt</code> and its dependecies, we will use the following command;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># yum install libvirt python-virtinst libvirt-daemon-xen
</span></span></code></pre></div><p>Restart <code>dom0</code> machine to make it active.</p>
<ol start="6">
<li>Now install <code>virt-manager</code> which is a GUI based virtual machine manager interface for Xen;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># yum install virt-manager
</span></span><span style="display:flex;"><span># /etc/init.d/libvirtd start
</span></span></code></pre></div><ol start="7">
<li>Since we need to access these virtual machines from other virtual machine hosted in other machines, we need to have a bridged network connection for the guest machines and use them as default interface for guest machine. To do so, we will copy the current configuration from <code>ifcfg-eth0</code> and will make necessary changes to create a bridged network;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># cp /etc/sysconfig/network-scrpit/ifcfg-eth0 /etc/sysconfig/network-scrpit/ifcfg-br0
</span></span><span style="display:flex;"><span># vi /etc/syconfig/network-script/ifcfg-br0
</span></span><span style="display:flex;"><span>DEVICE=br0
</span></span><span style="display:flex;"><span>TYPE=Bridge
</span></span><span style="display:flex;"><span>vi /etc/sysconfig/network-scrpt/ifcfg-eth0
</span></span><span style="display:flex;"><span>BRIDGE=br0
</span></span></code></pre></div><p>And</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># vi /etc/sysconfig/network-scrpit/ifcfg-eth0 /etc/sysconfig/network-scrpit/ifcfg-eth0
</span></span><span style="display:flex;"><span>BRIDGE=br0
</span></span></code></pre></div><ol start="8">
<li>Now we need to restart the network service;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># /etc/init.d/network restart
</span></span></code></pre></div><ol start="9">
<li>Xen is ready and we can start installing virtual machines there.</li>
</ol>
<h3 id="2-installing-virtual-machine-for-glusterfscephclient">
  2. Installing virtual machine for GlusterFS/Ceph/Client
  <a class="heading-link" href="#2-installing-virtual-machine-for-glusterfscephclient">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>Based on the above Host machine configuration, we need to create 8 virtual machine out of which 3 machines will be used for GlusterFS and 3 machines will be used for Ceph File System. Remaining 2 machines will be used as client machines. Out of these 8 machines, 7 of them will have CentOS 6.5 and one of the client machine will have Ubuntu 14.04.
Below is a sample configuration used to create 7 CentOS virtual machines;</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># virt-install \ 
</span></span><span style="display:flex;"><span>--name=server1 \ 
</span></span><span style="display:flex;"><span>--file=/var/lib/libvirt/images/gfs1.img \ 
</span></span><span style="display:flex;"><span>--file-size=100 \ 
</span></span><span style="display:flex;"><span>--nonsparse 
</span></span><span style="display:flex;"><span>--vcpus=1  
</span></span><span style="display:flex;"><span>--ram=2048 \ 
</span></span><span style="display:flex;"><span>--cdrom /dev/sr0
</span></span><span style="display:flex;"><span>--network bridge:br0 \ 
</span></span><span style="display:flex;"><span>--os-type=linux \ 
</span></span><span style="display:flex;"><span>--os-variant=rhel6
</span></span></code></pre></div><p>Below is the configuration used to create 1 Ubuntu virtual machine;</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># virt-install \ 
</span></span><span style="display:flex;"><span>--name=client2 \ 
</span></span><span style="display:flex;"><span>--file=/var/lib/libvirt/images/client2.img \ 
</span></span><span style="display:flex;"><span>--file-size=100 \ 
</span></span><span style="display:flex;"><span>--nonsparse 
</span></span><span style="display:flex;"><span>--vcpus=1  
</span></span><span style="display:flex;"><span>--ram=2048 \ 
</span></span><span style="display:flex;"><span>--cdrom /dev/sr0
</span></span><span style="display:flex;"><span>--network=bridge:br0 \ 
</span></span><span style="display:flex;"><span>--os-type=linux \ 
</span></span><span style="display:flex;"><span>--os-variant=ubuntuquantal
</span></span></code></pre></div><h3 id="3-post-installation-requirments">
  3. Post installation requirments
  <a class="heading-link" href="#3-post-installation-requirments">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<ol>
<li>Configure network for all of these three servers</li>
<li>Configure <code>/etc/host</code>s file so that they can resolve each other hostname to IP</li>
<li>Disable <code>Selinux</code></li>
<li>Install <code>openssh-clients</code> and <code>Vi</code> for work purpose</li>
</ol>
<hr>
<h2 id="c-installing-and-configuring-glusterfs">
  C. Installing and configuring GlusterFS
  <a class="heading-link" href="#c-installing-and-configuring-glusterfs">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<blockquote>
<p><em>gfs1 â Admin node + Gluster node 1</em><br>
<em>gfs2 â Gluster node 2 (Monitor daemon, Object storage)</em><br>
<em>gfs3 â Gluster node 3 (Monitor daemon, Object storage)</em><br>
<em>client1 â CentOS</em><br>
<em>client2 â Ubuntu</em></p>
</blockquote>
<h3 id="1-install-glusterfs-on-the-storage-servers">
  1. Install GlusterFS on the storage servers
  <a class="heading-link" href="#1-install-glusterfs-on-the-storage-servers">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<ol>
<li>Installing required packages since we have installed the minimal version of CentOS;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># yum install wget
</span></span></code></pre></div><ol start="2">
<li>At first, we will add GlusterFS repo in our repositories;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># wget -P /etc/yum.repos.d http://download.gluster.org/pub/gluster/glusterfs/LATEST/CentOS/glusterfs-epel.repo
</span></span></code></pre></div><ol start="3">
<li>Now installing Gluster in those three servers one by one;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># yum install glusterfs glusterfs-server glusterfs-fuse 
</span></span></code></pre></div><ol start="4">
<li>GlusterFS is installed, now start it for the first time;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># /etc/init.d/glusterd start
</span></span></code></pre></div><ol start="5">
<li>Check the installed version of the GlusterFS;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># glusterfsd --version
</span></span><span style="display:flex;"><span>glusterfs x.x.x built on Apr 23 2014 12:53:55
</span></span><span style="display:flex;"><span>Repository revision: git://git.gluster.com/glusterfs.git
</span></span><span style="display:flex;"><span>Copyright (c) 2006-2013 Red Hat, Inc. http://www.redhat.com/
</span></span></code></pre></div><ol start="6">
<li>Finally, we want GlusterFS to start automatically at startup. To do that, we will use the following command;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>#  chkconfig glusterfsd on
</span></span></code></pre></div><h3 id="2-installing-glusterfs-on-the-client-machines">
  2. Installing GlusterFS on the client machines
  <a class="heading-link" href="#2-installing-glusterfs-on-the-client-machines">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<ol>
<li>The client only require glusterfs and glusterfs-fuse to communicate with the servers;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># yum -y install glusterfs glusterfs-fuse
</span></span></code></pre></div><ol start="2">
<li>Now check the installed version of the GlusterFS;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># glusterfsd --version
</span></span><span style="display:flex;"><span>glusterfs x.x.x built on Jun 24 2014 15:09:41
</span></span><span style="display:flex;"><span>Repository revision: git://git.gluster.com/glusterfs.git
</span></span><span style="display:flex;"><span>Copyright (c) 2006-2013 Red Hat, Inc. http://www.redhat.com/
</span></span></code></pre></div><h3 id="3-creating-a-trusted-pool-among-the-storage-servers">
  3. Creating a trusted pool among the storage servers
  <a class="heading-link" href="#3-creating-a-trusted-pool-among-the-storage-servers">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<ol>
<li>To create a trusted pool, we have to make sure that all these servers can resolve each other hostname, we will manually enter this information in the <code>/etc/hosts</code> file to map IP to hostname;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># vi nano /etc/hosts
</span></span><span style="display:flex;"><span>192.168.26.230    server1
</span></span><span style="display:flex;"><span>192.168.26.231    server2
</span></span><span style="display:flex;"><span>192.168.26.232    server3
</span></span><span style="display:flex;"><span>192.168.26.233    client
</span></span><span style="display:flex;"><span>192.168.26.234    gfs1
</span></span><span style="display:flex;"><span>192.168.26.235    gfs2
</span></span><span style="display:flex;"><span>192.168.26.236    gfs3
</span></span><span style="display:flex;"><span>192.168.26.237    ceph1
</span></span><span style="display:flex;"><span>192.168.26.238    ceph2
</span></span><span style="display:flex;"><span>192.168.26.239    ceph3
</span></span></code></pre></div><ol start="2">
<li>We need to restart the network service to make this change effective;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># /etc/init.d/network restart
</span></span></code></pre></div><ol start="3">
<li>Now we will probe from <code>gfs1</code> to <code>gfs2</code> and from <code>gfs1</code> to <code>gfs3</code>. We are not going to probe l<code>ocahost/gfs1</code>, since we are running these commands from this machines;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># gluster peer probe gfs2
</span></span><span style="display:flex;"><span>Probe successful
</span></span><span style="display:flex;"><span># gluster peer probe gfs3
</span></span><span style="display:flex;"><span>Probe successful
</span></span></code></pre></div><ol start="4">
<li>Finally, verify the newly created trusted pool and check if all the servers are participating or not;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># gluster peer status
</span></span><span style="display:flex;"><span>Number of Peers: 2
</span></span><span style="display:flex;"><span>Hostname: gfs2
</span></span><span style="display:flex;"><span>Uuid: 1d6711ad-880c-48de-a81d-9bfa06adf775
</span></span><span style="display:flex;"><span>State: Peer in Cluster (Connected)
</span></span><span style="display:flex;"><span>Hostname: gfs3
</span></span><span style="display:flex;"><span>Uuid: cbda474e-efd7-4bc8-9c6c-4456bf4ec1b6
</span></span><span style="display:flex;"><span>State: Peer in Cluster (Connected)
</span></span></code></pre></div><h3 id="4-creating-a-distributed-volume-in-the-trusted-pool">
  4. Creating a distributed volume in the trusted pool
  <a class="heading-link" href="#4-creating-a-distributed-volume-in-the-trusted-pool">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<ol>
<li>At first, check if TCP connection between three servers are functional or not;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># netstat -tap | grep glusterfsd
</span></span><span style="display:flex;"><span>tcp        0      0 *:49152                     *:*                         LISTEN      3858/glusterfsd     
</span></span><span style="display:flex;"><span>tcp        0      0 gfs1:exp1                   gfs1:24007                  ESTABLISHED 3858/glusterfsd     
</span></span><span style="display:flex;"><span>tcp        0      0 gfs1:49152                  gfs2:1016                   ESTABLISHED 3858/glusterfsd     
</span></span><span style="display:flex;"><span>tcp        0      0 gfs1:49152                  gfs1:1020                   ESTABLISHED 3858/glusterfsd     
</span></span><span style="display:flex;"><span>tcp        0      0 gfs1:49152                  gfs3:1016                   ESTABLISHED 3858/glusterfsd 
</span></span><span style="display:flex;"><span># netstat -tap | grep glusterfsd
</span></span><span style="display:flex;"><span>tcp        0      0 *:49152                     *:*                         LISTEN      3716/glusterfsd     
</span></span><span style="display:flex;"><span>tcp        0      0 gfs2:exp1                   gfs2:24007                  ESTABLISHED 3716/glusterfsd     
</span></span><span style="display:flex;"><span>tcp        0      0 gfs2:49152                  gfs2:1020                   ESTABLISHED 3716/glusterfsd     
</span></span><span style="display:flex;"><span>tcp        0      0 gfs2:49152                  gfs3:1015                   ESTABLISHED 3716/glusterfsd     
</span></span><span style="display:flex;"><span>tcp        0      0 gfs2:49152                  gfs1:1014                   ESTABLISHED 3716/glusterfsd 
</span></span><span style="display:flex;"><span># netstat -tap | grep glusterfsd
</span></span><span style="display:flex;"><span>tcp        0      0 *:49152                     *:*                         LISTEN      3733/glusterfsd     
</span></span><span style="display:flex;"><span>tcp        0      0 gfs3:exp1                   gfs3:24007                  ESTABLISHED 3733/glusterfsd     
</span></span><span style="display:flex;"><span>tcp        0      0 gfs3:49152                  gfs1:1013                   ESTABLISHED 3733/glusterfsd     
</span></span><span style="display:flex;"><span>tcp        0      0 gfs3:49152                  gfs3:1020                   ESTABLISHED 3733/glusterfsd     
</span></span><span style="display:flex;"><span>tcp        0      0 gfs3:49152                  gfs2:1015                   ESTABLISHED 3733/glusterfsd     
</span></span></code></pre></div><ol start="2">
<li>Now create the first volume, <code>dist-volume</code> for all these three servers;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># gluster volume create dist-volume gfs1:/dist1 gfs2:/dist2 gfs3:/dist3
</span></span><span style="display:flex;"><span>volume create: dist-volume: success: please start the volume to access data
</span></span></code></pre></div><ol start="3">
<li>Volume has created without any error, now start the volume;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># gluster volume start dist-volume
</span></span><span style="display:flex;"><span>volume start: dist-volume: success
</span></span></code></pre></div><ol start="4">
<li>Finally, verify volume information;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># gluster volume info
</span></span><span style="display:flex;"><span>Volume Name: dist-volume
</span></span><span style="display:flex;"><span>Type: Distribute
</span></span><span style="display:flex;"><span>Volume ID: 6bc03033-1df5-45a6-ba9e-aa34786df129
</span></span><span style="display:flex;"><span>Status: Started
</span></span><span style="display:flex;"><span>Number of Bricks: 3
</span></span><span style="display:flex;"><span>Transport-type: tcp
</span></span><span style="display:flex;"><span>Bricks:
</span></span><span style="display:flex;"><span>Brick1: gfs1:/dist1
</span></span><span style="display:flex;"><span>Brick2: gfs2:/dist2
</span></span><span style="display:flex;"><span>Brick3: gfs3:/dist3
</span></span></code></pre></div><ol start="5">
<li>Now, we need to verify if this distributed volume, <code>dist-volume</code> is actually working as it supposed to or not. From <code>client1</code>, we will mount this volume and then randomly create some files, download some files from the internet and check if these files are distributed among those three servers. At first, we need to create a directory in the client machine to mount the disk and then mount it;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># mkdir /mnt/distributed
</span></span><span style="display:flex;"><span># mount.glusterfs gfs1:/dist-volume /mnt/distributed/
</span></span></code></pre></div><ol start="6">
<li>Verify if it is mounted properly;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># mount
</span></span><span style="display:flex;"><span>/dev/xvda2 on / type ext4 (rw)
</span></span><span style="display:flex;"><span>proc on /proc type proc (rw)
</span></span><span style="display:flex;"><span>sysfs on /sys type sysfs (rw)
</span></span><span style="display:flex;"><span>devpts on /dev/pts type devpts (rw,gid=5,mode=620)
</span></span><span style="display:flex;"><span>tmpfs on /dev/shm type tmpfs (rw)
</span></span><span style="display:flex;"><span>/dev/xvda1 on /boot type ext4 (rw)
</span></span><span style="display:flex;"><span>none on /proc/sys/fs/binfmt_misc type binfmt_misc (rw)
</span></span><span style="display:flex;"><span>gfs1:/dist-volume on /mnt/distributed type fuse.glusterfs (rw,default_permissions,allow_other,max_read=131072)
</span></span></code></pre></div><ol start="7">
<li>Check the available disk space for <code>client1</code>;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># df -h
</span></span><span style="display:flex;"><span>Filesystem         Size  Used Avail Use% Mounted on
</span></span><span style="display:flex;"><span>/dev/xvda2          95G  918M   89G   2% /
</span></span><span style="display:flex;"><span>tmpfs              938M     0  938M   0% /dev/shm
</span></span><span style="display:flex;"><span>/dev/xvda1         485M   52M  409M  12% /boot
</span></span><span style="display:flex;"><span>gfs1:/dist-volume  279G  2.8G  262G   2% /mnt/distributed
</span></span></code></pre></div><ol start="8">
<li>Now create some files from <code>client1</code> machine and check if distribution is working amount the servers;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># touch test1
</span></span><span style="display:flex;"><span># touch test2
</span></span><span style="display:flex;"><span># touch test3
</span></span><span style="display:flex;"><span># touch test4
</span></span><span style="display:flex;"><span># touch test5
</span></span><span style="display:flex;"><span># touch test6
</span></span><span style="display:flex;"><span># touch test7
</span></span><span style="display:flex;"><span># touch test8
</span></span><span style="display:flex;"><span># touch test9
</span></span><span style="display:flex;"><span># touch test10
</span></span><span style="display:flex;"><span># wget https://www.kernel.org/pub/linux/kernel/v3.x/linux-3.15.1.tar.xz
</span></span></code></pre></div><ol start="9">
<li>Check if the files are successfully created;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># ls -l
</span></span><span style="display:flex;"><span>total 77813
</span></span><span style="display:flex;"><span>-rw-r--r-- 1 root root 79679864 Jun 16 16:54 linux-3.15.1.tar.xz
</span></span><span style="display:flex;"><span>-rw-r--r-- 1 root root        0 Jun 25 07:23 test1
</span></span><span style="display:flex;"><span>-rw-r--r-- 1 root root        0 Jun 25 07:24 test10
</span></span><span style="display:flex;"><span>-rw-r--r-- 1 root root        0 Jun 25 07:23 test2
</span></span><span style="display:flex;"><span>-rw-r--r-- 1 root root        0 Jun 25 07:23 test3
</span></span><span style="display:flex;"><span>-rw-r--r-- 1 root root        0 Jun 25 07:23 test4
</span></span><span style="display:flex;"><span>-rw-r--r-- 1 root root        0 Jun 25 07:23 test5
</span></span><span style="display:flex;"><span>-rw-r--r-- 1 root root        0 Jun 25 07:24 test6
</span></span><span style="display:flex;"><span>-rw-r--r-- 1 root root        0 Jun 25 07:24 test7
</span></span><span style="display:flex;"><span>-rw-r--r-- 1 root root        0 Jun 25 07:24 test8
</span></span><span style="display:flex;"><span>-rw-r--r-- 1 root root        0 Jun 25 07:24 test9
</span></span></code></pre></div><ol start="10">
<li>Finally, check from all these three servers to see if these 11 files are distributed or not;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># ls -l /dist1/
</span></span><span style="display:flex;"><span>total 0
</span></span><span style="display:flex;"><span>-rw-r--r-- 2 root root 0 Jun 25 07:24 test10
</span></span><span style="display:flex;"><span>-rw-r--r-- 2 root root 0 Jun 25 07:23 test3
</span></span><span style="display:flex;"><span>-rw-r--r-- 2 root root 0 Jun 25 07:24 test6
</span></span><span style="display:flex;"><span>-rw-r--r-- 2 root root 0 Jun 25 07:24 test7
</span></span><span style="display:flex;"><span># ls -l /dist2/
</span></span><span style="display:flex;"><span>total 77820
</span></span><span style="display:flex;"><span>-rw-r--r-- 2 root root 79679864 Jun 16 16:54 linux-3.15.1.tar.xz
</span></span><span style="display:flex;"><span>-rw-r--r-- 2 root root        0 Jun 25 07:23 test1
</span></span><span style="display:flex;"><span>-rw-r--r-- 2 root root        0 Jun 25 07:23 test2
</span></span><span style="display:flex;"><span>-rw-r--r-- 2 root root        0 Jun 25 07:23 test4
</span></span><span style="display:flex;"><span>-rw-r--r-- 2 root root        0 Jun 25 07:23 test5
</span></span><span style="display:flex;"><span>-rw-r--r-- 2 root root        0 Jun 25 07:24 test9
</span></span><span style="display:flex;"><span># ls -l /dist3/
</span></span><span style="display:flex;"><span>total 0
</span></span><span style="display:flex;"><span>-rw-r--r-- 2 root root 0 Jun 25 07:24 test8
</span></span></code></pre></div><ol start="11">
<li>We can see that all the files are distributed among all three storage servers.</li>
</ol>
<h3 id="5-creating-a-replicated-volume-in-the-trusted-pool">
  5. Creating a replicated volume in the trusted pool
  <a class="heading-link" href="#5-creating-a-replicated-volume-in-the-trusted-pool">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<ol>
<li>Create the first replicated volume, <code>rep-volume</code> for all these three servers;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># gluster volume create rep-volume replica 3 gfs1:/rep1 gfs2:/rep2 gfs3:/rep3 force
</span></span><span style="display:flex;"><span>volume create: rep-volume: success: please start the volume to access data
</span></span></code></pre></div><ol start="2">
<li>Volume has created without any error, now start the volume;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># gluster volume start rep-volume
</span></span><span style="display:flex;"><span>volume start: rep-volume: success
</span></span></code></pre></div><ol start="3">
<li>Finally, verify volume information;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># gluster volume start rep-volume
</span></span><span style="display:flex;"><span>volume start: rep-volume: success
</span></span><span style="display:flex;"><span># gluster volume info rep-volume
</span></span><span style="display:flex;"><span>Volume Name: rep-volume
</span></span><span style="display:flex;"><span>Type: Replicate
</span></span><span style="display:flex;"><span>Volume ID: f7765efe-163f-42ab-9fd9-41df18db0f9c
</span></span><span style="display:flex;"><span>Status: Started
</span></span><span style="display:flex;"><span>Number of Bricks: 1 x 3 = 3
</span></span><span style="display:flex;"><span>Transport-type: tcp
</span></span><span style="display:flex;"><span>Bricks:
</span></span><span style="display:flex;"><span>Brick1: gfs1:/rep1
</span></span><span style="display:flex;"><span>Brick2: gfs2:/rep2
</span></span><span style="display:flex;"><span>Brick3: gfs3:/rep3
</span></span></code></pre></div><ol start="4">
<li>Now, we need to verify if this replicated volume, <code>rep-volume</code> is actually working as it supposed to or not. From client1, we will mount this volume and then randomly create some files and check if its completed replicated among those three servers or not. At first, we need to create a directory in the client machine to mount the disk and then mount it;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># mkdir /mnt/replicated
</span></span><span style="display:flex;"><span># mount.glusterfs gfs1:/rep-volume /mnt/replicated/
</span></span></code></pre></div><ol start="5">
<li>Verify if it is mounted properly;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># mount
</span></span><span style="display:flex;"><span>/dev/xvda2 on / type ext4 (rw)
</span></span><span style="display:flex;"><span>proc on /proc type proc (rw)
</span></span><span style="display:flex;"><span>sysfs on /sys type sysfs (rw)
</span></span><span style="display:flex;"><span>devpts on /dev/pts type devpts (rw,gid=5,mode=620)
</span></span><span style="display:flex;"><span>tmpfs on /dev/shm type tmpfs (rw)
</span></span><span style="display:flex;"><span>/dev/xvda1 on /boot type ext4 (rw)
</span></span><span style="display:flex;"><span>none on /proc/sys/fs/binfmt_misc type binfmt_misc (rw)
</span></span><span style="display:flex;"><span>gfs1:/dist-volume on /mnt/distributed type fuse.glusterfs (rw,default_permissions,allow_other,max_read=131072)
</span></span><span style="display:flex;"><span>gfs1:/rep-volume on /mnt/replicated type fuse.glusterfs (rw,default_permissions,allow_other,max_read=131072)
</span></span></code></pre></div><ol start="6">
<li>Check the available disk space for client1;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># df -h
</span></span><span style="display:flex;"><span>Filesystem         Size  Used Avail Use% Mounted on
</span></span><span style="display:flex;"><span>/dev/xvda2          95G  918M   89G   2% /
</span></span><span style="display:flex;"><span>tmpfs              938M     0  938M   0% /dev/shm
</span></span><span style="display:flex;"><span>/dev/xvda1         485M   52M  409M  12% /boot
</span></span><span style="display:flex;"><span>gfs1:/dist-volume  279G  2.8G  262G   2% /mnt/distributed
</span></span><span style="display:flex;"><span>gfs1:/rep-volume    91G  926M   85G   2% /mnt/replicated
</span></span></code></pre></div><ol start="7">
<li>Now create some files from client1 machine and check if distribution is working amount the servers;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># touch rep1
</span></span><span style="display:flex;"><span># touch rep2
</span></span><span style="display:flex;"><span># touch rep3
</span></span><span style="display:flex;"><span># touch rep4
</span></span></code></pre></div><ol start="8">
<li>Check if the files are successfully created;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># ls -l
</span></span><span style="display:flex;"><span>total 77813
</span></span><span style="display:flex;"><span>-rw-r--r-- 1 root root 79679864 Jun 16 16:54 linux-3.15.1.tar.xz
</span></span><span style="display:flex;"><span>-rw-r--r-- 1 root root        0 Jun 25 07:23 test1
</span></span><span style="display:flex;"><span>-rw-r--r-- 1 root root        0 Jun 25 07:24 test10
</span></span><span style="display:flex;"><span>-rw-r--r-- 1 root root        0 Jun 25 07:23 test2
</span></span><span style="display:flex;"><span>-rw-r--r-- 1 root root        0 Jun 25 07:23 test3
</span></span><span style="display:flex;"><span>-rw-r--r-- 1 root root        0 Jun 25 07:23 test4
</span></span><span style="display:flex;"><span>-rw-r--r-- 1 root root        0 Jun 25 07:23 test5
</span></span><span style="display:flex;"><span>-rw-r--r-- 1 root root        0 Jun 25 07:24 test6
</span></span><span style="display:flex;"><span>-rw-r--r-- 1 root root        0 Jun 25 07:24 test7
</span></span><span style="display:flex;"><span>-rw-r--r-- 1 root root        0 Jun 25 07:24 test8
</span></span><span style="display:flex;"><span>-rw-r--r-- 1 root root        0 Jun 25 07:24 test9
</span></span></code></pre></div><ol start="9">
<li>Now check from all these three servers to see if these 4 files are replicated or not;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># ls -l /rep1/
</span></span><span style="display:flex;"><span>total 0
</span></span><span style="display:flex;"><span>-rw-r--r-- 2 root root 0 Jun 25 07:43 rep1
</span></span><span style="display:flex;"><span>-rw-r--r-- 2 root root 0 Jun 25 07:43 rep2
</span></span><span style="display:flex;"><span>-rw-r--r-- 2 root root 0 Jun 25 07:43 rep3
</span></span><span style="display:flex;"><span>-rw-r--r-- 2 root root 0 Jun 25 07:43 rep4
</span></span><span style="display:flex;"><span># ls -l /rep2/
</span></span><span style="display:flex;"><span>total 0
</span></span><span style="display:flex;"><span>-rw-r--r-- 2 root root 0 Jun 25 07:43 rep1
</span></span><span style="display:flex;"><span>-rw-r--r-- 2 root root 0 Jun 25 07:43 rep2
</span></span><span style="display:flex;"><span>-rw-r--r-- 2 root root 0 Jun 25 07:43 rep3
</span></span><span style="display:flex;"><span>-rw-r--r-- 2 root root 0 Jun 25 07:43 rep4
</span></span><span style="display:flex;"><span># ls -l /rep3/
</span></span><span style="display:flex;"><span>total 0
</span></span><span style="display:flex;"><span>-rw-r--r-- 2 root root 0 Jun 25 07:43 rep1
</span></span><span style="display:flex;"><span>-rw-r--r-- 2 root root 0 Jun 25 07:43 rep2
</span></span><span style="display:flex;"><span>-rw-r--r-- 2 root root 0 Jun 25 07:43 rep3
</span></span><span style="display:flex;"><span>-rw-r--r-- 2 root root 0 Jun 25 07:43 rep4
</span></span></code></pre></div><ol start="10">
<li>We can see that all the files are distributed among all three servers.</li>
</ol>
<h3 id="6-creating-a-stripped-volume-in-the-trusted-pool">
  6. Creating a stripped volume in the trusted pool
  <a class="heading-link" href="#6-creating-a-stripped-volume-in-the-trusted-pool">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<ol>
<li>Now create the first replicated volume, <code>strip-volume</code> for all these three servers;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># gluster volume create strip-volume strip 3 gfs1:/strip1 gfs2:/strip2 gfs3:/strip3 force
</span></span><span style="display:flex;"><span>volume create: strip-volume: success: please start the volume to access data
</span></span></code></pre></div><ol start="2">
<li>Volume has created without any error, now start the volume;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># gluster volume start strip-volume
</span></span><span style="display:flex;"><span>volume start: strip-volume: success
</span></span></code></pre></div><ol start="3">
<li>Finally, verify volume information;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># gluster volume start strip-volume
</span></span><span style="display:flex;"><span>volume start: rep-volume: success
</span></span><span style="display:flex;"><span># gluster volume info strip-volume
</span></span><span style="display:flex;"><span>Volume Name: strip-volume
</span></span><span style="display:flex;"><span>Type: Stripe
</span></span><span style="display:flex;"><span>Volume ID: 2eb5e39e-c891-4cc9-be5d-1315b284e0e1
</span></span><span style="display:flex;"><span>Status: Started
</span></span><span style="display:flex;"><span>Number of Bricks: 1 x 3 = 3
</span></span><span style="display:flex;"><span>Transport-type: tcp
</span></span><span style="display:flex;"><span>Bricks:
</span></span><span style="display:flex;"><span>Brick1: gfs1:/strip1
</span></span><span style="display:flex;"><span>Brick2: gfs2:/strip2
</span></span><span style="display:flex;"><span>Brick3: gfs3:/strip3
</span></span></code></pre></div><ol start="4">
<li>Now, we need to verify if this stripped volume, <code>strip-volume</code> is actually working as it supposed to or not. From client1, we will mount this volume and then create a large file and check if its stripped among those three servers or not. At first, we need to create a directory in the client machine to mount the disk and then mount it;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># mkdir /mnt/stripped
</span></span><span style="display:flex;"><span># mount.glusterfs gfs1:/strip-volume /mnt/stripped/
</span></span></code></pre></div><ol start="5">
<li>Verify if it is mounted properly;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># mount
</span></span><span style="display:flex;"><span>/dev/xvda2 on / type ext4 (rw)
</span></span><span style="display:flex;"><span>proc on /proc type proc (rw)
</span></span><span style="display:flex;"><span>sysfs on /sys type sysfs (rw)
</span></span><span style="display:flex;"><span>devpts on /dev/pts type devpts (rw,gid=5,mode=620)
</span></span><span style="display:flex;"><span>tmpfs on /dev/shm type tmpfs (rw)
</span></span><span style="display:flex;"><span>/dev/xvda1 on /boot type ext4 (rw)
</span></span><span style="display:flex;"><span>none on /proc/sys/fs/binfmt_misc type binfmt_misc (rw)
</span></span><span style="display:flex;"><span>gfs1:/dist-volume on /mnt/distributed type fuse.glusterfs (rw,default_permissions,allow_other,max_read=131072)
</span></span><span style="display:flex;"><span>gfs1:/rep-volume on /mnt/replicated type fuse.glusterfs (rw,default_permissions,allow_other,max_read=131072)
</span></span><span style="display:flex;"><span>gfs1:/strip-volume on /mnt/stripped type fuse.glusterfs (rw,default_permissions,allow_other,max_read=131072
</span></span></code></pre></div><ol start="6">
<li>Check the available disk space for client1;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># df -h
</span></span><span style="display:flex;"><span>Filesystem          Size  Used Avail Use% Mounted on
</span></span><span style="display:flex;"><span>/dev/xvda2           95G  918M   89G   2% /
</span></span><span style="display:flex;"><span>tmpfs               938M     0  938M   0% /dev/shm
</span></span><span style="display:flex;"><span>/dev/xvda1          485M   52M  409M  12% /boot
</span></span><span style="display:flex;"><span>gfs1:/dist-volume   279G  2.8G  262G   2% /mnt/distributed
</span></span><span style="display:flex;"><span>gfs1:/rep-volume     91G  926M   85G   2% /mnt/replicated
</span></span><span style="display:flex;"><span>gfs1:/strip-volume  279G  2.8G  262G   2% /mnt/stripped
</span></span></code></pre></div><ol start="7">
<li>We are going to create a 100 GB file and check if its stripped amount these three servers or not;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># dd if=/dev/zero of=/mnt/stripped/stripped-image.img bs=102400k count=1000
</span></span><span style="display:flex;"><span>1000+0 records in
</span></span><span style="display:flex;"><span>1000+0 records out
</span></span><span style="display:flex;"><span>104857600000 bytes (105 GB) copied, 1014.37 s, 103 MB/s
</span></span></code></pre></div><ol start="8">
<li>Now check from all these three servers to see if these 4 files are replicated or not;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># ls -lh /strip1/
</span></span><span style="display:flex;"><span>total 33G
</span></span><span style="display:flex;"><span>-rw-r--r-- 2 root root 33G Jun 25 12:48 stripped-image.img
</span></span><span style="display:flex;"><span># ls -lh /strip2/
</span></span><span style="display:flex;"><span>total 33G
</span></span><span style="display:flex;"><span>-rw-r--r-- 2 root root 33G Jun 25 12:49 stripped-image.img
</span></span><span style="display:flex;"><span># ls -lh /strip3/
</span></span><span style="display:flex;"><span>total 33G
</span></span><span style="display:flex;"><span>-rw-r--r-- 2 root root 33G Jun 25 12:51 stripped-image.img
</span></span></code></pre></div><ol start="9">
<li>We can see that all the files are stripped among all three servers.</li>
</ol>
<hr>
<h2 id="d-installing-and-configuring-cephfs">
  D. Installing and configuring CephFS
  <a class="heading-link" href="#d-installing-and-configuring-cephfs">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<blockquote>
<p><em>ceph1 â Admin node</em><br>
<em>ceph2 â Ceph node 1 (Monitor daemon, Object storage)</em><br>
<em>ceph3 â Ceph node 2 (Monitor daemon, Object storage)</em><br>
<em>client1 â CentOS</em><br>
<em>client2 â Ubuntu</em></p>
</blockquote>
<h3 id="1-install-cephfs-on-the-storage-servers">
  1. Install CephFS on the storage servers
  <a class="heading-link" href="#1-install-cephfs-on-the-storage-servers">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<ol>
<li>Create a new Ceph user and set password;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># useradd âd /home/ceph âm ceph
</span></span><span style="display:flex;"><span># passwd ceph
</span></span></code></pre></div><ol start="2">
<li>Give this new user root previliges;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># echo âceph ALL = (root) NOPASSWD:ALLâ | sudo tee /etc/sudoers.d/ceph
</span></span><span style="display:flex;"><span># chmod 440 /etc/sudoers.d/ceph
</span></span></code></pre></div><ol start="3">
<li>Add Ceph repositiory to CentOS machine;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># sudo nano /etc/yum.repos.d/ceph.repo
</span></span></code></pre></div><p>And add the following lines</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>[ceph-noarch]
</span></span><span style="display:flex;"><span>name=Ceph noarch packages
</span></span><span style="display:flex;"><span>baseurl=http://ceph.com/rpm-firefly/el/noarch
</span></span><span style="display:flex;"><span>enabled=1
</span></span><span style="display:flex;"><span>gpgcheck=1
</span></span><span style="display:flex;"><span>type=rpm-md
</span></span><span style="display:flex;"><span>gpgkey=https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc
</span></span></code></pre></div><p>Update the repository.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># sudo yum update
</span></span></code></pre></div><ol start="4">
<li>Install Ceph;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># sudo yum install ceph-deploy
</span></span></code></pre></div><ol start="4">
<li>Create unattented SSH access between ceph1, ceph2 and ceph3;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># ssh-keygen
</span></span></code></pre></div><p>Edit SSH config file to allow all cluster to talk with each other;</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># nano ~./ssh/config
</span></span></code></pre></div><p>Add the following line in the file;</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>Host ceph1
</span></span><span style="display:flex;"><span>Hostname ceph1
</span></span><span style="display:flex;"><span>User ceph
</span></span><span style="display:flex;"><span>Host ceph2
</span></span><span style="display:flex;"><span>Hostname ceph2
</span></span><span style="display:flex;"><span>User ceph
</span></span><span style="display:flex;"><span>Host ceph02
</span></span><span style="display:flex;"><span>Hostname ceph3
</span></span><span style="display:flex;"><span>User ceph
</span></span></code></pre></div><p>Give permission to this file;</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># chmod 600 ~/.ssh/config
</span></span></code></pre></div><p>Now copy this SSH public key to all the nodes;</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># ssh-copy-id ceph1
</span></span><span style="display:flex;"><span># ssh-copy-id ceph2
</span></span><span style="display:flex;"><span># ssh-copy-id ceph3
</span></span></code></pre></div><ol start="5">
<li>Create directory for Ceph Cluster in the Admin node;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># mkdir ceph-cluster
</span></span><span style="display:flex;"><span># cd ceph-cluster
</span></span></code></pre></div><ol start="6">
<li>Configure Ceph Cluster;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># ceph-deploy new ceph1 ceph2 ceph3
</span></span></code></pre></div><ol start="7">
<li>Deploy Ceph on all nodes;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># ceph-deploy install ceph1 ceph2 ceph3
</span></span></code></pre></div><ol start="8">
<li>Disable <code>cephx</code> authentication which is used between to nodes to authenticate each other. In this way we can consume less operational overhead;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># nano /home/ceph/ceph-cluster/ceph.conf
</span></span><span style="display:flex;"><span>Change this file to the following;
</span></span><span style="display:flex;"><span>auth_server_required=none
</span></span><span style="display:flex;"><span>auth_cluster_required=none
</span></span><span style="display:flex;"><span>mon_clock_drift_allowed=10
</span></span></code></pre></div><p>Now restart Ceph;</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># sudo /etc/init.d/ceph restart
</span></span></code></pre></div><ol start="9">
<li>Install configuration for monitoring and keys;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># ceph-deploy âoverwrite-conf mon create-initial
</span></span></code></pre></div><ol start="10">
<li>Create directories in all the storage node where Ceph will be mounted;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># mkdir /home/ceph/ceph-storage1
</span></span><span style="display:flex;"><span># mkdir /home/ceph/ceph-storage2
</span></span><span style="display:flex;"><span># mkdir /home/ceph/ceph-storage3
</span></span></code></pre></div><ol start="11">
<li>Prepare Object Storage Daemon;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># ceph-deploy osd prepare ceph1:/ceph-storage1 ceph2:/ceph-storage2 ceph3:/ceph-storage3
</span></span></code></pre></div><ol start="12">
<li>Activate Object Storage Daemon;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># ceph-deploy osd activate ceph1:/ceph-storage1 ceph2:/ceph-storage2 ceph3:/ceph-storage3
</span></span></code></pre></div><ol start="13">
<li>Configure Meta Data Server;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># ceph-deploy admin ceph1
</span></span><span style="display:flex;"><span># ceph-deploy mds create ceph1
</span></span></code></pre></div><ol start="14">
<li>Check MDS Status;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># ceph mds stat
</span></span><span style="display:flex;"><span>E32: 1/1/1 up {0=ceph=up:active}
</span></span><span style="display:flex;"><span># ceph health
</span></span></code></pre></div><ol start="15">
<li>Configure NTP to sync between nodes;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># yum install ntp
</span></span><span style="display:flex;"><span># ntpdate
</span></span></code></pre></div><ol start="16">
<li><em>Special note:</em> All CentOS machine have to add ceph in require tty like this;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># sudo visudo
</span></span></code></pre></div><p>And change the following line;</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>Defaults:ceph !requiretty
</span></span></code></pre></div><h3 id="2-ceph-client-installation">
  2. Ceph Client Installation
  <a class="heading-link" href="#2-ceph-client-installation">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<ol>
<li>From Admin node (ceph1), install ceph client;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># ceph-deploy install client1 client2
</span></span></code></pre></div><ol start="2">
<li>From Admin node (ceph1), copy Ceph configuration file and keyring to client1 and client2;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># ceph-deploy admin client1 client2
</span></span></code></pre></div><ol start="3">
<li>From client node (client1), create a Block Device Image of 100 GB to Ceph file system;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># rbd create storage1 âsize 102400
</span></span></code></pre></div><ol start="4">
<li>Map the image to a block device</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># sudo rbd map storage1
</span></span></code></pre></div><ol start="5">
<li>Check RBD mapping;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># rbd showmapped
</span></span><span style="display:flex;"><span>id 	pool	image	snap	device
</span></span><span style="display:flex;"><span>1	rbd	storage1    -	/dev/rbd1
</span></span></code></pre></div><ol start="6">
<li>Use this block device and create an <code>ext4</code> file system;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># sudo mkfs.ext4 /dev/rbd/rbd/storage1
</span></span></code></pre></div><ol start="7">
<li>Mount the file system in client machine;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># sudo mkdir /mnt/storage1
</span></span><span style="display:flex;"><span># sudo mount /dev/rbd/rbd/storage1 /mnt/storage1
</span></span></code></pre></div><ol start="8">
<li>Check the file system;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># df âh
</span></span></code></pre></div><ol start="9">
<li>Do step 3-8 for client2 and here Block Device Image will be <code>storage2</code> and mount point will be <code>/mnt/storage2</code>.</li>
</ol>
<hr>
<h2 id="e-installing-benchmark-tools">
  E. Installing Benchmark Tools
  <a class="heading-link" href="#e-installing-benchmark-tools">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<h3 id="1-installing-iozone">
  1. Installing iozone
  <a class="heading-link" href="#1-installing-iozone">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<ol>
<li>Since we are using 64-bit operating system, it will be good if we use 64-bit version of iozone. But iozone only provides 32-bit version of RPM package, so we will use Source RPM, compile it to a 64-bit RPM package and then install it. Initially, we will install <code>rpm-build</code>, <code>make</code> and <code>gcc</code> to resolve the dependencies for <code>rpmbuild</code>;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># yum install rpm-build
</span></span><span style="display:flex;"><span># yum install make
</span></span><span style="display:flex;"><span># yum install gcc
</span></span></code></pre></div><ol start="2">
<li>Now we will download iozone source RPM from the authors site and rebuild it according to our OS architecture;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># cd /tmp/
</span></span><span style="display:flex;"><span># wget http://iozone.org/src/current/iozone-3-424.src.rpm
</span></span><span style="display:flex;"><span># rpmbuild --rebuild iozone-3-424.src.rpm
</span></span></code></pre></div><ol start="3">
<li>The newly build RPM package will be stored in the following location, from there we can install the package;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># cd ~/rpmbuild/RPMS/x86_64/
</span></span><span style="display:flex;"><span># rpm -ivh iozone-3-424.x86_64.rpm 
</span></span></code></pre></div><ol start="4">
<li>Finally, iozone is installed in the following location and we can run iozone from here;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># cd /opt/iozone/bin/
</span></span><span style="display:flex;"><span># ./iozone âa
</span></span></code></pre></div><ol start="5">
<li>For client2 (Ubuntu OS), iozone is available in the default repository;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># apt-get install iozone3
</span></span></code></pre></div><h3 id="2-installing-bonnie">
  2. Installing Bonnie++
  <a class="heading-link" href="#2-installing-bonnie">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<ol>
<li>Bonnie++ does not come with the default CentOS repo list. So we have to frist add <code>RPMForge</code> repository and then we can install it;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># rpm --import http://apt.sw.be/RPM-GPG-KEY.dag.txt
</span></span><span style="display:flex;"><span># rpm -K rpmforge-release-0.5.3-1.el6.rf.*.rpm
</span></span><span style="display:flex;"><span># rpm -i rpmforge-release-0.5.3-1.el6.rf.*.rpm
</span></span></code></pre></div><ol start="2">
<li>Now we can install Bonnie++ using yum;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># yum install bonnie++
</span></span></code></pre></div><ol start="3">
<li>For client2, Bonnie++ is available in the default repository;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># apt-get install bonnie++
</span></span></code></pre></div><hr>
<h2 id="f-results">
  F. Results
  <a class="heading-link" href="#f-results">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<h3 id="1-iozone">
  1. IOzone
  <a class="heading-link" href="#1-iozone">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<p>Create a 100 GB File using iozone;</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span># iozone /mnt/distributed/10GB-dist.img ât 4 âr 512k -s 10g âi 0 âi 1 âi 2 | tee âa /root/client1/Distributed/iozone-10g-512k.txt
</span></span></code></pre></div><p><code>/mnt/distributed/10GB-dist.img</code> â we are using the Gluster distributed file system mounted in the client machines to create dummy files (such as, iozone.DUMMY.1, iozone.DUMMY.2, iozone.DUMMY.3 iozone.DUMMY.4) to measure the throughput.</p>
<p>[Parameters]</p>
<p><code>r</code> â Record size or Block size, since most of the spinning disk has a block size of 512k, we have also tested with 1024k and 2048k<br>
<code>t</code> â indicates iozone to run in throughput mode, we have also used 4 threads/processes during the measurement<br>
<code>s</code> â size of the file to test, here 10-dist.img has a file size of 10 GB, so we are using this file to test the throughput<br>
<code>i â 0</code> - write and rewrite<br>
<code>i - 1</code> - read and reread<br>
<code>1 - 2</code> - random read and random write</p>
<p>We are exporting the result in the txt file to create graph afterwordâs.</p>
<p><em>Total number of testing in IOzone:</em><br>
â¢ iozone-100m-512k.txt<br>
â¢ iozone-100m-1024.txt<br>
â¢ iozone-100m-2048.txt<br>
â¢ iozone-1g-512k.txt<br>
â¢ iozone-1g-1024.txt<br>
â¢ iozone-1g-2048.txt<br>
â¢ iozone-4g-512k.txt<br>
â¢ iozone-4g-1024.txt<br>
â¢ iozone-4g-2048.txt<br>
â¢ iozone-8g-512k.txt<br>
â¢ iozone-8g-1024.txt<br>
â¢ iozone-8g-2048.txt<br>
â¢ iozone-10g-512k.txt<br>
â¢ iozone-10g-1024.txt<br>
â¢ iozone-10g-2048.txt</p>
<p>We cannot completely rely on testing <code>1GB-dist.img</code> because iozone start buffereing while file size is less than the memory, so we will only focus on 4GB, 8GB and 10GB measurement and 1GB will be only used for reference.</p>
<h4 id="11-iozone-performance-in-glusterfs">
  1.1 iozone performance in GlusterFS
  <a class="heading-link" href="#11-iozone-performance-in-glusterfs">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h4>
<p>In this experiment, we are using the Gluster file system distributed volume which is mounted in the client machine and this directory has used to create dummy files (for example iozone.DUMMY.1, iozone.DUMMY.2 etc) for I/O benchmarking. We have used different file block size (512K, 1024K, 2048K) and different file size (1GB, 4GB, 8GB, 10GB) to get more diverse results. All the testing is run in throughput mode with 4 threads/process and we have taken the average throughput per process. In Figure-1 and Figure-2 we have seen that, file operations such as Write, Read, Re-write, Re-read, Random Write are better in Client1 than Client2 where Random read operation is still bad on both machines. Also note to mention that Client2 works better on Write and Re-write operation than other file operations. In Random read operation, the bigger the file block size, the performance gets better.</p>
<p><em>GlusterFS CentOS Client</em></p>
<p><img src="http://ntrezowan.github.com/images/gfs1-iozone.jpg" alt="GlusterFS CentOS Client"></p>
<p><em>GlusterFS Ubuntu Client</em></p>
<p><img src="http://ntrezowan.github.com/images/gfs2-iozone.jpg" alt="GlusterFS Ubuntu Client"></p>
<h4 id="12-iozone-performance-in-cephfs">
  1.2 iozone performance in CephFS
  <a class="heading-link" href="#12-iozone-performance-in-cephfs">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h4>
<p><em>CephFS CentOS Client</em></p>
<p><img src="http://ntrezowan.github.com/images/cfs1-iozone.jpg" alt="Ceph CentOS Client"></p>
<p><em>CephFS Ubuntu Client</em></p>
<p><img src="http://ntrezowan.github.com/images/cfs2-iozone.jpg" alt="Ceph Ubuntu Client"></p>
<h3 id="2-dd">
  2. dd
  <a class="heading-link" href="#2-dd">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h3>
<h4 id="21-glusterfs-testbed">
  2.1 GlusterFS Testbed
  <a class="heading-link" href="#21-glusterfs-testbed">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h4>
<h5 id="211-client1">
  2.1.1 client1
  <a class="heading-link" href="#211-client1">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h5>
<ol>
<li>Creating 1GB of endless stream of zero bytes to /mnt/distributed with block size=1024k and repeat this 1000 times;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>#  dd if=/dev/zero of=/mnt/distributed/1GB-dist.img bs=1024k count=1000 oflag=direct
</span></span></code></pre></div><ol start="2">
<li>Creating 4GB of endless stream of zero bytes to /mnt/distributed with block size=1024k and repeat this 4000 times;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>#  dd if=/dev/zero of=/mnt/distributed/4GB-dist.img bs=1024k count=4000 oflag=direct
</span></span></code></pre></div><ol start="3">
<li>Creating 8GB of endless stream of zero bytes to /mnt/distributed with block size=1024k and repeat this 8000 times;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>#  dd if=/dev/zero of=/mnt/distributed/8GB-dist.img bs=1024k count=8000 oflag=direct
</span></span></code></pre></div><ol start="4">
<li>Creating 10GB of endless stream of zero bytes to /mnt/distributed with block size=1024k and repeat this 10000 times;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>#  dd if=/dev/zero of=/mnt/distributed/10GB-dist.img bs=1024k count=10000 oflag=direct
</span></span></code></pre></div><h5 id="212-client2">
  2.1.2 client2
  <a class="heading-link" href="#212-client2">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h5>
<ol>
<li>Creating 1GB of endless stream of zero bytes to /mnt/distributed with block size=1024k and repeat this 1000 times;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>#  dd if=/dev/zero of=/mnt/distributed/1GB-dist.img bs=1024k count=1000 oflag=direct
</span></span></code></pre></div><ol start="2">
<li>Creating 4GB of endless stream of zero bytes to /mnt/distributed with block size=1024k and repeat this 4000 times;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>#  dd if=/dev/zero of=/mnt/distributed/4GB-dist.img bs=1024k count=4000 oflag=direct
</span></span></code></pre></div><ol start="3">
<li>Creating 8GB of endless stream of zero bytes to /mnt/distributed with block size=1024k and repeat this 8000 times;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>#  dd if=/dev/zero of=/mnt/distributed/8GB-dist.img bs=1024k count=8000 oflag=direct
</span></span></code></pre></div><ol start="4">
<li>Creating 10GB of endless stream of zero bytes to /mnt/distributed with block size=1024k and repeat this 10000 times;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>#  dd if=/dev/zero of=/mnt/distributed/10GB-dist.img bs=1024k count=10000 oflag=direct
</span></span></code></pre></div><h4 id="22-cephfs-testbed">
  2.2 CephFS Testbed
  <a class="heading-link" href="#22-cephfs-testbed">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h4>
<h5 id="221-client1">
  2.2.1 client1
  <a class="heading-link" href="#221-client1">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h5>
<ol>
<li>Creating 1GB of endless stream of zero bytes to /mnt/distributed with block size=1024k and repeat this 1000 times;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>#  sudo dd if=/dev/zero of=/mnt/distributed/1GB-dist.img bs=1024k count=1000 oflag=direct
</span></span></code></pre></div><ol start="2">
<li>Creating 4GB of endless stream of zero bytes to /mnt/distributed with block size=1024k and repeat this 4000 times;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>#  sudo dd if=/dev/zero of=/mnt/distributed/4GB-dist.img bs=1024k count=4000 oflag=direct
</span></span></code></pre></div><ol start="3">
<li>Creating 8GB of endless stream of zero bytes to /mnt/distributed with block size=1024k and repeat this 8000 times;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>#  sudo dd if=/dev/zero of=/mnt/distributed/8GB-dist.img bs=1024k count=8000 oflag=direct
</span></span></code></pre></div><ol start="4">
<li>Creating 10GB of endless stream of zero bytes to /mnt/distributed with block size=1024k and repeat this 10000 times;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>#  sudo dd if=/dev/zero of=/mnt/distributed/10GB-dist.img bs=1024k count=10000 oflag=direct
</span></span></code></pre></div><h5 id="222-client2">
  2.2.2 client2
  <a class="heading-link" href="#222-client2">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h5>
<ol>
<li>Creating 1GB of endless stream of zero bytes to /mnt/distributed with block size=1024k and repeat this 1000 times;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>#  sudo dd if=/dev/zero of=/mnt/distributed/1GB-dist.img bs=1024k count=1000 oflag=direct
</span></span></code></pre></div><ol start="2">
<li>Creating 4GB of endless stream of zero bytes to /mnt/distributed with block size=1024k and repeat this 4000 times;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>#  sudo dd if=/dev/zero of=/mnt/distributed/4GB-dist.img bs=1024k count=4000 oflag=direct
</span></span></code></pre></div><ol start="3">
<li>Creating 8GB of endless stream of zero bytes to /mnt/distributed with block size=1024k and repeat this 8000 times;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>#  sudo dd if=/dev/zero of=/mnt/distributed/8GB-dist.img bs=1024k count=8000 oflag=direct
</span></span></code></pre></div><ol start="4">
<li>Creating 10GB of endless stream of zero bytes to /mnt/distributed with block size=1024k and repeat this 10000 times;</li>
</ol>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>#  sudo dd if=/dev/zero of=/mnt/distributed/10GB-dist.img bs=1024k count=10000 oflag=direct
</span></span></code></pre></div><h4 id="23-dd-performance-for-glusterfs-and-cephfs">
  2.3 dd Performance for GlusterFS and CephFS
  <a class="heading-link" href="#23-dd-performance-for-glusterfs-and-cephfs">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h4>
<p><em>dd performance</em></p>
<p><img src="http://ntrezowan.github.com/images/dd.jpg" alt="dd performance"></p>
<hr>
<h2 id="g-summary">
  G. Summary
  <a class="heading-link" href="#g-summary">
    <i class="fa fa-link" aria-hidden="true" title="Link to heading"></i>
    <span class="sr-only">Link to heading</span>
  </a>
</h2>
<p>Ceph and Gluster file system emerge good technologies to the storage system even if their internal structure are different. Gluster file system performance appear practically solid than Ceph file system which is still in development stage. Both file system supports data de-duplication with fail-over and load balancing which is expected in clustered storage. Ceph file system can be installed and managed very efficiently because a single node interface is capable of managing the entire cluster where in Gluster file system, every node have to managed separately for different purposes. Both file system can be deployed in virtual environments and in cloud computing platform from where clients can manage their user space. These two file system are open-source which gives alternative solution to expensive storage system and also supports most of the inexpenvive and commodity hardware.</p>

      </div>


      <footer>
        


        
        
        
        
      </footer>
    </article>

    
  </section>

    </div>

    <footer class="footer">
  <section class="container">
    Â©
    
    2023
     John De 
    Â·
    
    Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/">Coder</a>.
    
  </section>
</footer>

  </main>

  

  
  
  <script src="/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js" integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script>
  

  

  


  

  

  

  

  

  

  

  

  
</body>

</html>
